%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS

\input{setup.tex}

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{
Supplementary Material for
``LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems''}
%\title{LUCID -- ???}
% Black-Box Verification Engine for Embodied AI Systems?
% \author {
%     % Authors
    % Ernesto Casablanca\textsuperscript{\rm 1},
    % Oliver Sch\"on\textsuperscript{\rm 1}%,
    % Third Author Name\textsuperscript{\rm 2}
% }
% \affiliations {
    % Affiliations
    % \textsuperscript{\rm 1}Newcastle University, United Kingdom\\
    % \textsuperscript{\rm 2}Affiliation 2\\
    % e.casablanca2@ncl.ac.uk, o.schoen2@ncl.ac.uk%, thirdAuthor@affiliation1.com
% }


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
    Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical.
    Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics.
    To address these challenges, we introduce \lucid (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions.
    As such, \lucid is the first known tool capable of establishing quantified safety guarantees for such systems.
    Thanks to its modular architecture and extensive documentation, \lucid is designed for easy extensibility.

    \lucid employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees.
    We use conditional mean embeddings to embed data into a \gls{rkhs}, where an \gls{rkhs} ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior.

    A key innovation within \lucid is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program.
    The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety.
    \lucid thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety.
    These unique capabilities are demonstrated on challenging benchmarks.
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
\begin{links}
    \link{Source Code}{https://gitlab.com/lucidtoolsource/lucid}
    \link{Documentation}{https://lucidtoolsource.gitlab.io/lucid/}
    % \link{Datasets}{https://aaai.org/example/datasets}
    % \link{Extended version}{https://aaai.org/example/extended-version}
\end{links}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Embodied forms of AI are on the rise, including applications such as autonomous vehicles, robotics, personalized healthcare, and smart infrastructure.
Their core functionality is built around the advances of deep learning, enabling systems to understand and reason in complex and human-like ways to produce a desired behavior.
Whilst the black-box nature of AI components has been a crucial contributor to the widespread success of deep learning, in response to a rapidly evolving legal scrutinization~\cite{veale2021demystifying}, the resulting lack of traceability and certifiability has put a premature halt to its deployment in safety-critical applications.

As deep learning agents are left to choose their actions autonomously in closed-loop interaction with the physical world, they are confronted with a world riddled with randomness and uncertainty.
Efforts to establishing trust in their safe operation have been largely focused on developing tools to verify the input--output behavior of \glspl{nn} embedded in the systems~\cite{liu2021algorithms}.
Unfortunately, there do not exist any tools that could take these results and certify the safety of the entire system, as uncertainty-aware models of closed-loop systems are rarely available and simulators are often too complicated and opaque to perform verification directly~\cite{wongpiromsarn2023formal}. This motivates a holistic black-box treatment~\cite{corso2021survey}, where safety guarantees are to be established from behavioral data and in account of the unknown laws of randomness themselves.

There exist no tools capable of quantifying safety guarantees for complex stochastic closed-loop systems from data.
%
For the setting where a model of the closed-loop system is available, \npinterval by \citet{harapanahalli2023forward} verifies safety of nonlinear systems with non-deterministic disturbance based on existing \gls{nn} verification tools. See the references therein and the annual friendly competition by \citet{abate2024arch} for adjacent work.
%
Few data-driven approaches for stochastic systems exist.
\omnisafe is a comprehensive platform for the development of safe \gls{rl} algorithms~\cite{ji2024omnisafe}. However, safe \gls{rl} does generally only \emph{encourage} safer behavior, without providing any rigorous or quantified guarantees on the probability of the absence of unsafe behavior.
As a popular working principle for certifying safety, \glspl{cbc}~\cite{prajna2006barrier} are at the basis of tools such as \trust~\cite{gardner2025trust}, which supports only polynomial dynamics, and \fossil~\cite{edwards2024fossil}, which is model-based, and both being restricted to deterministic systems.


To close this gap (see Table~\ref{tab:qualitative_comparison}), we introduce \lucid, the first verification engine for black-box stochastic dynamical systems with quantified guarantees.
At its core, \lucid learns \glspl{cbc} for unknown systems based solely on data, by constructing an uncertainty-aware estimator of the expected system behavior based on \glspl{cme}~\cite{muandet2017kernel}.
Crucially, the underlying learning framework establishes quantified safety probabilities with distributionally robust guarantees for arbitrary smooth dynamics, thus relaxing the need for restrictive structural assumptions common to related approaches.
For instance, \citet{schoen2024distributionally} learn \glspl{cbc} for systems with polynomial dynamics and \citet{chen2025distributionally} assume systems with known deterministic component.
\begin{table}
    \centering
    \begin{tabular}{ccccccc}
        \toprule
        \textbf{Tool}                                    & \multicolumn{6}{c}{\textbf{Supported Features}}                                                                                                                                                                  \\[.4em]
                                                         & \rotatebox{90}{Guarantees}                      & \rotatebox{90}{Data Driven} & \rotatebox{90}{Stochastic Dyn.}  & \rotatebox{90}{Non-Poly. Dyn.} & \rotatebox{90}{Stat. Correct.} & \rotatebox{90}{Closed Loop} \\
        \midrule
        \textbf{\lucid} (this work)                      & \cmark                                          & \cmark                      & \cmark                           & \cmark                         & \cmark                         & \cmark                      \\
        \trust~\shortcite{gardner2025trust}              & \cmark                                          & \cmark                      & \xmarkred                        & \xmarkred                      & \cmark\footnotemark[1]         & \cmark                      \\
        \fossil~\shortcite{edwards2024fossil}            & \cmark                                          & \xmarkred                   & \xmarkred                        & \cmark                         & NA                             & \cmark                      \\
        \omnisafe~\shortcite{ji2024omnisafe}             & \xmarkred                                       & \cmark                      & \cmark                           & \cmark                         & \xmarkred                      & \cmark                      \\
        \npinterval~\shortcite{harapanahalli2023forward} & \cmark                                          & \xmarkred                   & \cmark/\xmarkred\footnotemark[2] & \cmark                         & NA                             & \cmark                      \\
        % \deepsplit~\shortcite{henriksen2021deepsplit} & \xmarkred & \xmarkred & \cmark & \xmarkred& \xmarkred \\
        % \reluplex~\shortcite{katz2017reluplex} & \xmarkred & \xmarkred & \cmark & \xmarkred & \xmarkred \\
        % tool6                                            &                                                 &                             &                               &                                                                                             \\
        \bottomrule
    \end{tabular}
    \caption{Qualitative comparison with existing tools based on their supported features: quantified safety guarantees, data driven, stochastic dynamics, non-polynomial dynamics, statistical correctness guarantees of the learned model (only applicable if data driven), and support for closed-loop systems.}
    \label{tab:qualitative_comparison}
\end{table}
\footnotetext[1]{Assuming the data satisfies persistence of excitation.} % TRUST
\footnotetext[2]{Accepts non-deterministic bounded disturbances.} % npinterval

%
Alternative approaches often leave the statistical correctness with respect to the underlying data-generating process unaddressed: \citet{kazemi2020fullLTL} use model-free reinforcement learning, relying on known Lipschitz constants; \citet{lew2021sampling} compute reachable sets for stochastic systems using a sampling-based scheme, which yields only asymptotic guarantees; and \citet{salamati2024data} use a scenario-based method, based on Lipschitz constants and exponentially large datasets.
For data-driven safety verification of stochastic systems via conformal prediction~\cite{lindemann2024formal}, no tools are available.



We summarize the main contributions of this work:
\begin{itemize}
    \item We introduce \lucid, a novel verification engine that learns control barrier certificates from data using kernel-based \glspl{cme}. \lucid uses a tractable reformulation of the problem via a finite Fourier expansion, enabling efficient barrier synthesis based on a linear program.
    \item A robust and extensible software implementation, with both C++ and Python interfaces, supporting configuration via YAML, JSON, or Python scripts, and offering both a \gls{cli} and a \gls{gui}.
    \item Numerical evaluation on a suite of benchmarks, demonstrating \lucid{}â€™s unique capabilities, robustness, and practical utility.
\end{itemize}




\paragraph{Organization}
The paper is organized as follows.
Section~\ref{sec:theory} provides the theoretical foundations of \lucid, including the safety of black-box dynamical systems, control barrier certificates, conditional mean embeddings, and derivation of the relaxed linear program.
Section~\ref{sec:tool} describes the architecture and functionalities of \lucid alongside a running example, presenting \lucid's components and how they interact.
% Section~\ref{sec:example} presents a simple example use case, demonstrating how to use \lucid to certify the safety of a simple linear system.
Section~\ref{sec:experiments} evaluates \lucid on a suite of benchmarks. Finally, Section~\ref{sec:conclusion_and_future_extensions} provides a conclusion and future extensions.

\section{Theoretical Working Principles}\label{sec:theory}
\subsection{Safety of Black-Box Dynamical Systems}

\paragraph{System Description}
Many AI-driven systems exhibit complex, nonlinear, and stochastic behavior that can be modeled as discrete-time stochastic processes with Markovian dynamics:
\begin{equation}
    \label{eq:model}
    \S\colon\quad x_{t+1}  = f(x_t,a_t,w_t),\quad w_t\sim p_w,
\end{equation}
where $f\colon\X\times\A\times\W\rightarrow\X$ is a continuous vector field describing the evolution of the system state $x_t\in\X\subset\R^n$ over time $t\in\smash{\N_{\geq0}}$, driven by control actions $a_t\in\A\subset\R^m$ and process noise $w_t\in\W\subset\R^l$.
The noise is assumed to be drawn from a stochastic distribution $p_w$ in an independent and identically distributed (i.i.d.) manner.
%
This general formulation subsumes a wide range of systems, including discrete-time \glspl{mdp}.


\paragraph{Black-Box Policies}
Here, the focus is on systems $\S$ driven by black-box control policies of the form $\pi\colon\R^n\rightarrow\R^m$, i.e., at every time step $t=0,1,2,\ldots$ a continuous action $a_t\in\A$ is selected based on the current state $x_t$. Such policies could be, for example, neural networks trained via \gls{rl}, or any other black-box function generating actions in $\R^m$ (see Figure~\ref{fig:closed_loop_system}).
The resulting closed-loop system is denoted as $\S^\pi$.
\begin{figure}
    \centering
    \begin{tikzpicture}
        \foreach \i in {4, ..., 1}{
                \node[name=xa\i] at (\i/4*9,0) {$x_\i$};
            }
        \foreach \i in {4, ..., 1}{
                \node[name=aa\i] at (\i/4*9+.5,0) {$u_{\i}$};
            }
        \foreach \i in {4, ..., 1}{
                \path[draw,->] (xa\i) -- +(0,.5)-| node[left,above,xshift = -0.3cm]{\footnotesize$\pi$} (aa\i);
            }
        \foreach \i in {3, ..., 1}{
                \pgfmathtruncatemacro{\j}{\i + 1}
                \path[draw,->] (aa\i)-- node[above]{\footnotesize$\S$} (xa\j);
            }
    \end{tikzpicture}
    \caption{Evolution of the closed-loop system.}
    \label{fig:closed_loop_system}
\end{figure}

\paragraph{Dataset}
Due to their complexity and opacity, such systems must often be treated as black boxes in their entirety, assuming only access to a finite amount, $N$, of system observations
\begin{equation}
    \data_N\colon \quad \{(x^i,a^i,x^i_+)\}_{i=1}^N,\label{eq:data}
\end{equation}
where every sampled transition from $x^i\in\X$ to a successor $x^i_+\in\X$ is generated as a realization
\begin{equation*}
    (x^i,a^i,x_+^i)\sim\int \delta_{f(x^i,a^i,w)}(dx_+^i)\,p_w(dw)\,\mathcal{U}_\X(dx^i)\,\mathcal{U}_\A(da^i),
\end{equation*}
with $\mathcal{U}_\X$ and $\mathcal{U}_\A$ uniform distributions on $\X$ and $\A$, respectively, and $\delta$ indicating the Dirac delta distribution capturing the system dynamics.

\input{figures/steps}

\paragraph{Safety Problem}
A question that often arises when designing a policy $\pi$ for a system $\S$, especially in engineering contexts, is if the closed-loop system $\S^\pi$ elicits safe behavior. That is, when starting from a domain $\X_{0}\subset\X$ under a policy $\pi$, the system $\S^\pi$ shall avoid unsafe regions $\X_U\subset\X$ (such as obstacles) for at least some predefined time horizon $T\in\N\cup\{\infty\}$.

%Such safety problems are tricky and even for the case where an explicit model \eqref{eq:model} is given, 
Certifying safety of a discrete-time stochastic system over an uncountable state space does not generally admit an analytical solution and is extremely challenging, especially for complex dynamics~\cite{abate2008probabilistic}.
Furthermore, for unbounded stochasticity $w_t\sim p_w$, there is generally no yes/no answer to safety, but a safety probability $P_{\text{safe}}(\S^\pi)\in[0,1)$. 
Note that given a policy $\pi$, for every initial state $x_0\in\X_0$ there is an associated safety probability. Our goal is to estimate a \emph{lower bound} on the infimum of such probabilities across all initial states in $\X_0$.
% Thus the goal is generally to estimate a \emph{lower bound} on the true safety probability $P_{\text{safe}}(\S^\pi)$. \textcolor{blue}{[Paolo] I think this needs clarification: given a policy $\pi$, for every initial state there is a safety probability. With Lucid we aim at computing a lower bound on the inf of such probabilities across all initial states. Did I understand correctly?}

\paragraph{Problem Statement}
Assuming access to a finite dataset $\data_N$ from the black-box system $\S$ in \eqref{eq:model}, quantify a certifiable lower bound on the probability of the black-box system $\S^\pi$ being safe with respect to a safety specification given by $(\X_0,\X_U,T)$.

\smallskip

Available sampling-based methods such as Monte Carlo simulation are unfit to solve this problem, as they compute guarantees for fixed initial conditions $x_0\in\X$.
\lucid provides a solution for continuous sets $\X_0$ by automating the steps in Figure~\ref{fig:steps}, outlined in the next sections.


\subsection{Control Barrier Certificates}\label{sec:cbc}
\glspl{cbc}\footnote[3]{\glspl{cbc} and discrete-time \glspl{cbf} \cite{cosner2023generative} are equivalent.} leverage the concept of set invariance to arrive at an abstraction-free numerical solution to finding a lower bound on $P_{\text{safe}}(\S^\pi)$. This has made them popular tools for safety verification and controller synthesis~\cite{prajna2006barrier}.

A non-negative function $\B\colon\X\rightarrow\R_{\geq 0}$ is a \gls{cbc} of a system $\S$ with reference to an unsafe set $\X_U$ if it satisfies
\begin{itemize}
    \item[(a)] $\forall x_0\in \X_0\colon\,\B(x_0)\leq\eta$;
    \item[(b)] $\forall x_U\in \X_U\colon\,\B(x_U)\geq\gamma$; and
    \item[(c)] $\forall x\in\X,\,\exists a\in\A\colon\,\E[\B(X_+) \mid X=x,\,A=a] -\B(x) \leq  c;$
\end{itemize}
for some constants $\gamma>\eta\geq0$ and $c\geq 0$. Here, upper case $X_+$, $X$, and $A$ denote the random variables underlying concrete realizations $(x_t,a_t,x_{t+1})$ elicited by $\S$.
%
Intuitively, if one can find a \gls{cbc} for a system $\S$, then, a lower bound on the probability of $\S$ being safe can be quantified based on the distance between the two level sets $\gamma$ and $\eta$~\cite{kushner1967stochastic}:
\begin{equation}
    P_{\text{safe}}(\S^\pi)\geq 1- \frac{\eta + cT}{\gamma},\label{eq:safety_prob_lb}
\end{equation}
where $T$ is the desired time horizon.
Notably, the system dynamics only enters the barrier conditions through the conditional expectation in (c), and a \gls{cbc} thus only depends on the \emph{expected} system behavior, with no need to consider its concrete stochastic law.

Whilst \eqref{eq:safety_prob_lb} can provide a robust assessment of a system's safety, in practice, the bound can be overly conservative and thus several improved variants of the original barrier constraints exist~\cite{mahathi2022kinductive}.
As these conditions in essence all rely on the computation of chance constraint with respect to the expected behavior of the system, they are basically interchangeable.

Although there exist model-\emph{based} efficient solutions to this problem for linear and control affine systems, this is generally a semi-infinite problem, demanding a data-driven solution. Furthermore, for the data-driven case, establishing constraint (c) rigorously without relying on impractical assumptions is extremely challenging.

\subsection{Data-Driven Dynamics Estimation via Conditional Mean Embeddings}\label{sec:dd_estimation_via_cme}
To reason about the expected value of a random variable, embedding the variable into a (higher dimensional) space and forming a data-driven estimate is a well-established concept in machine learning \cite{scholkopf2002learning,Steinwart2008SVM}.
Following the same reasoning, the \emph{kernel mean embedding} represents the projection of a probability measure into a \gls{rkhs} via a feature map $\phi$ associated with a positive definite kernel $k$~\cite{Smola2007EmbedDistrb,muandet2017kernel}.
For conditional probability distributions a similar concept exists: \glspl{cme} can be used to model the expected value of any \gls{rkhs} function $g\colon\X\rightarrow\R$ under a stochastic process such as $\S$~\cite{park2020measuretheoretic,muandet2017kernel}.
For the Gaussian kernel,
\begin{equation}
    k(x,x') := \sigma_f^2 \exp\left( -\tfrac{1}{2} (x-x')\T \Sigma\, (x-x') \right),\label{eq:gaussian_kernel}
\end{equation}
where $\Sigma:=\diag(\sigma_l)^{-2}$, with hyperparameters $\sigma_f,\sigma_l\in\R$, the associated RKHS encompasses all smooth functions $g$.
A data-driven estimate can then be obtained in closed form from a finite amount of data $\data_N$:
\begin{align}
    \begin{split}
         & \E[f(X_+)\mid X=x,\,A=a] \approx                                                  \\
        % \innerH{f}{\hat{\Psi}^N_{X^+|X}(x)}{\Hilbert_{k}} = 
         & \hspace{5em} k_{XA}^N(x,a)\T\left[ K_{XA}^N+N \lambda I_N\right]^{-1}\! f(X_+^N),
    \end{split}\label{eq:empiricalEstimate}
\end{align}
with column vector $k_{XA}^N(x,a):=[k((x^i,a^i),(x,a))]_{i=1}^N$, Gram matrix $K_{XA}^N:=[k((x^i,a^i),(x^j,a^j))]_{i,j=1}^N$, regularization factor $\lambda\geq0$, identity matrix $I_N$, and $f(X_+^N):=[f(x^i_+)]_{i=1}^N$.
The empirical estimator in \eqref{eq:empiricalEstimate} converges in expectation to the true \gls{cme} for $N\rightarrow\infty$~\cite{park2020measuretheoretic}.

It is common practice to robustify empirical estimates such as \eqref{eq:empiricalEstimate} to out-of-sample behavior by constructing an \gls{rkhs} ambiguity set centered at the empirical \gls{cme}. The result is a distributionally robust estimator with an adjustable robustness radius. The details are omitted here for brevity, but the interested reader is referred to the works by \citet{kuhn2025distributionally} and \citet{li2022optimal}.


\subsection{Data-Driven Spectral Barriers}\label{sec:ddbarriers}
Based on the data-driven estimator in \eqref{eq:empiricalEstimate}, the problem of computing \glspl{cbc} using data can be formulated as a nonconvex semi-infinite program, which for general classes of systems and barriers is extremely difficult to solve.
To arrive at a tractable solution, \lucid conducts two additional steps:
\paragraph{1. Spectral Abstraction} Inspired by the popular random Fourier features approach by \citet{rahimi2007random}, the Gaussian kernel \eqref{eq:gaussian_kernel} admits a Fourier expansion
\begin{align}
    k(x, x') \equiv \sigma_f^2 \int_{\R^{n}}\!\! \mathcal{N}(d\omega\mid0,\Sigma)\; e^{\mathbf{i}\omega\T (P(x)-P(x'))},\label{eq:sqexp_kernel_fourier}
\end{align}
with the zero-mean Gaussian distribution $\smash{\mathcal{N}(d\omega\mid0,\Sigma)}$ with covariance $\Sigma$, the imaginary unit $\mathbf{i}:=\smash{\sqrt{-1}}$, and where the projection $x \mapsto P(x)$ maps the domain $\X$ into the unit hypercube $[0,1]^{n}$.
Partitioning the space of spatial frequencies $\omega\in\R^n$ into a set of discrete frequency bands (see Figure~\ref{fig:spectral_barrier}) yields a spectral abstraction of the associated \gls{rkhs}, i.e., characterizing learnable functions $\B$ in the form of Fourier series.
By truncating the series to a finite number, $M$, of fixed frequency bands $\omega_j\in\R^{n}$, $j\in\{0,\ldots,M\}$, the resulting barriers are of the form
\begin{align*}
    \B(x) = \alpha_0 +\sum_{i=1}^{M} \alpha_i \cos\left(\omega_i\T P(x)\right) + \beta_i \sin\left(\omega_i\T P(x)\right).%\label{eq:blackbox:representer_form_spectral}
\end{align*}
Notably, $\B$ admits the linear form $\B(x)=\phi_M(x)\T b$, with a truncated Fourier feature map $\phi_M\colon\X\rightarrow\R^{2M+1}$, parametrized by learnable spectral amplitudes
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fourier_series.pdf}
    \caption{Spectral barrier certificate $\B(x)=\phi_M(x)\T b$.}
    \vspace{-0.3cm}
    \label{fig:spectral_barrier}
\end{figure}
\begin{align*}
    b\!=\!\begin{bmatrix}
              \frac{\alpha_0}{\sigma_f^2\w_0^2}\!\! & \frac{\alpha_1}{2\sigma_f^2\w_1^2}\!\!\! & \frac{\beta_1}{2\sigma_f^2\w_1^2}\!\!\! & \ldots\!\!\! & \frac{\alpha_{M}}{2\sigma_f^2\w_{M}^2}\!\!\! & \frac{\beta_{M}}{2\sigma_f^2\w_{M}^2}
          \end{bmatrix}\T\!\!\!\!\in\R^{2M+1}\!\!,
\end{align*}
where the weights $\w_0,\ldots,\w_{M}\in\R_{\geq 0}$ associated with each frequency band are determined efficiently from the kernel's Gaussian spectral measure via the multivariate CDF (see Figure~\ref{fig:spectral_measure_abstraction}).
The \gls{cme}-based estimator \eqref{eq:empiricalEstimate} is approximated in the same finite basis via $H\in\R^{(2M+1)\times (2M+1)}$ such that
\begin{equation*}
    k_{XA}^N(x,a)\T\left[ K_{XA}^N+N \lambda I_N\right]^{-1}\! \Phi_{M,+}^N
    \approx
    \varphi_M(x,a)\T H,
\end{equation*}
where $\Phi_{M,+}^N:=[\phi_M(x_+^i)\T]_{i=1}^N$, $\varphi_M(x,a)\colon\X\times\A\rightarrow\smash{\R^{2M+1}}$ a feature map augmenting $\phi_M(x)$, and
with an approximation error decreasing exponentially with $M$~\cite{rahimi2007random}.
This reduces the barrier synthesis problem
% This simplifies the barrier problem 
to a semi-infinite linear program, with barrier conditions linear in $b$:
\begin{itemize}
    \item[(a)] $\forall x_0\in \X_0\colon\,\phi_M(x_0)\T b\leq\eta$;
    \item[(b)] $\forall x_U\in \X_U\colon\,\phi_M(x_U)\T b\geq\gamma$; and
    \item[(c)] $\forall x\in\X,\,\exists a\in\A\colon\,\varphi_M(x,a)\T (Hb -b) \leq  c.$
\end{itemize}
For the case where $a$ is provided by a given fixed policy $\pi$, $\varphi_M(x,a)$ reduces to $\phi_M(x)$.
% \OS{Importantly, the closed-form estimator in \eqref{eq:empiricalEstimate} does not depend on the choice of features and only through the spectral abstraction is the feature map introduced directly.}
\begin{figure}
    \centering
    % GAUSSIANs: 68-95-99 rule
    \begin{tikzpicture}
        \def\N{50} % number of sample points
        \def\B{0};
        \def\Bs{3.0};
        \def\var{1.3}
        \def\xmax{\B+3.5*\Bs};
        \def\ymin{{-0.1*gauss(\B,\B,\Bs)}};
        \def\h{0.08*gauss(\B,\B,\Bs)};

        \begin{axis}[every axis plot post/.append style={
            mark=none,domain={-(\xmax)}:{1.0*\xmax},samples=\N,smooth},
            xmin={-(\xmax)}, xmax=\xmax,
            axis/.style={>=latex},
            ymin=\ymin, ymax={1.1*gauss(\B,\B,\Bs)},
            axis lines=middle,
            axis line style=thick,
            enlarge x limits, % extend the axes a bit
            ticks=none,
            xlabel=$\omega$,
            every axis x label/.style={at={(current axis.right of origin)},anchor=north},
            width=1.1\linewidth, height=0.55*\linewidth,
            y=700pt,
            clip=false,
            axis line style={-latex}
            ]

            % PLOTS
            \addplot[accent,thick,name path=B] {gauss(x,\B,\var*\Bs)};

            % FILL
            \path[name path=xaxis]
            (\B-\pgfkeysvalueof{/pgfplots/xmax},0) -- (\B+\pgfkeysvalueof{/pgfplots/xmax},0); %\pgfkeysvalueof{/pgfplots/xmin}
            \addplot[accent!3.75] fill between[of=xaxis and B, soft clip={domain={\B-3.5*\Bs}:{\B+3.5*\Bs}}];
            \addplot[accent!7.5] fill between[of=xaxis and B, soft clip={domain={\B-2.5*\Bs}:{\B+2.5*\Bs}}];
            \addplot[accent!15] fill between[of=xaxis and B, soft clip={domain={\B-1.5*\Bs}:{\B+1.5*\Bs}}];
            \addplot[accent!30] fill between[of=xaxis and B, soft clip={domain={\B-.5*\Bs}:{\B+.5*\Bs}}];

            % LINES
            \addplot[black,dashdotted,thin]
            coordinates {({\B-3*\Bs},{20*gauss(\B-3*\Bs,\B,\Bs)}) ({\B-3*\Bs},{-\h})}
            node[below=-3pt,scale=1.0] {\strut$\omega_3$};
            \addplot[black,dashdotted,thin]
            coordinates {({\B-2*\Bs},{4*gauss(\B-2*\Bs,\B,\Bs)}) ({\B-2*\Bs},{-\h})}
            node[below=-3pt,scale=1.0] {\strut$\omega_2$};
            \addplot[black,dashdotted,thin]
            coordinates {({\B-1*\Bs},{1.3*gauss(\B-\Bs,\B,\Bs)}) ({\B-1*\Bs},{-\h})}
            node[below=-3pt,scale=1.0] at ({\B-\Bs},{-\h}) {\strut$\omega_1$};
            \addplot[black,dashdotted,thin]
            coordinates {(\B,{1.05*gauss(\B,\B,\Bs)}) (\B,{-\h})}
            node[below=-3pt,scale=1.0] {\strut$\omega_0$};
            \node[scale=1.0] (w0)
            at ({\B+.5*\Bs},{.125}) {$\w_0^2$};
            \node (b0) at ({\B+.25*\Bs},{.08}) {};
            \draw[->] (w0) to (b0.center);
            % \addplot[white,dashed,thin]
            % coordinates {({\B+.5*\Bs},{gauss(\B+.5*\Bs,\B,\Bs)}) ({\B+.5*\Bs},0)};
            \addplot[black,dashdotted,thin]
            coordinates {({\B+1*\Bs},{1.3*gauss(\B+\Bs,\B,\Bs)}) ({\B+1*\Bs},{-\h})}
            node[below=-3pt,scale=1.0] at ({\B+\Bs},{-\h}) {\strut$\omega_1$};
            \node[scale=1.0] (w1)
            at ({\B+1.5*\Bs},{.085}) {$\frac{\w_1^2}{2}$};
            \node (b1) at ({\B+1.25*\Bs},{.04}) {};
            \draw[->] (w1) to (b1.center);
            \addplot[black,dashdotted,thin]
            coordinates {({\B+2*\Bs},{4*gauss(\B+2*\Bs,\B,\Bs)}) ({\B+2*\Bs},{-\h})}
            node[below=-3pt,scale=1.0] at ({\B+2*\Bs},{-\h}) {\strut$\omega_2$};
            \node[scale=1.0] (w2)
            at ({\B+2.5*\Bs},{.055}) {$\frac{\w_2^2}{2}$};
            \node (b2) at ({\B+2.25*\Bs},{.01}) {};
            \draw[->] (w2) to (b2.center);
            \addplot[black,dashdotted,thin]
            coordinates {({\B+3*\Bs},{20*gauss(\B+3*\Bs,\B,\Bs)}) ({\B+3*\Bs},{-\h})}
            node[below=-3pt,scale=1.0] at ({\B+3*\Bs},{-\h}) {\strut$\omega_3$};
            \node[scale=1.0] (w3)
            at ({\B+3.5*\Bs},{.046}) {$\frac{\w_3^2}{2}$};
            \node (b3) at ({\B+3.25*\Bs},{.001}) {};
            \draw[->] (w3) to (b3.center);

            % Hyperparameters
            \addplot[<->,accent]
            coordinates {({\B-2*\var*\Bs},{gauss(\B+2*\var*\Bs,\B,\var*\Bs)}) ({\B+2*\var*\Bs},{gauss(\B+2*\var*\Bs,\B,\var*\Bs)})};
            \node[accent,fill=accent!30,inner xsep=3,inner ysep=2,scale=.8] at (\B,{gauss(\B+2*\var*\Bs,\B,\var*\Bs)}) {$\pm 2\sigma_l$};
            \addplot[<->,accent]
            coordinates {({\B-3*\var*\Bs},{gauss(0,0,\var*\Bs)}) ({\B-3*\var*\Bs},{0})};
            \addplot[dashed,thin,accent]
            coordinates {({\B-3*\var*\Bs},{gauss(0,0,\var*\Bs)}) ({0},{gauss(0,0,\var*\Bs)})};
            \node[accent,fill=white,inner xsep=3,inner ysep=2,scale=.8] at (\B-3*\var*\Bs,{.5*gauss(0,0,\var*\Bs)}) {$\sigma_f^2$};

        \end{axis}
    \end{tikzpicture}
    % \includegraphics[width=\linewidth]{figures/spectral_measure_abstraction.png}
    \caption{Abstraction of a 1-dim. Gaussian spectral measure of the Gaussian kernel, shown in \eqref{eq:sqexp_kernel_fourier}.}
    \vspace{-0.5cm}
    \label{fig:spectral_measure_abstraction}
\end{figure}
% Naturally, barriers of the form \eqref{eq:blackbox:representer_form_spectral} will be referred to as \emph{Fourier control barrier certificates}.

\paragraph{2. Finite-Constraint Relaxation} To obtain a program with \emph{finitely} many constraints, trigonometric bounding results \cite{pfister2018bounding} are used to obtain a relaxed linear program by sampling spatial lattices on $\X$, $\X_0$, and $\X_U$.
For example, to enforce barrier constraint (a) in Section~\ref{sec:cbc}, it suffices to construct a lattice $\{x_0^i\}_{i=1}^{N_0}\subset\X_0$ and impose the constraints $b\T\phi_M(x_0^i)\leq\eta$ for $i=1,\ldots,N_0$.
Selecting lattices with a sampling density meeting the Nyquist-Shannon sampling theorem with respect to the highest frequency $\omega_M$ appearing in the barrier and truncated estimator, the relaxation retains all guarantees.
As a result, the semi-infinite problem is relaxed to a finitely-constrained \gls{lp}, provided in full in the supplementary material.
Recall that given an appropriate robustness radius, the barriers produced by \lucid based on data $\data_N$ are statistically correct with respect to the unknown true system $\S$, and a lower bound for the safety probability is computed according to \eqref{eq:safety_prob_lb}.
%\Sadegh{Just to double check: we are not making the third inequality more conservative related to CME.} \OS{Not currently implemented.}

\section{Tool Structure and Functionalities}
\label{sec:tool}
\lucid implements the previously outlined functionality, written in C++ and designed to be used as a library or standalone executable.
The choice of a low level language gives us plenty of freedom and fine-grained control over the execution of the software.
We expose a set of interfaces allowing users to highly customize the verification process.
A high-level visualization of \lucid's architecture is illustrated in Figure~\ref{fig:architecture}, while a more technical description can be found in the online documentation at
\begin{center}
    \url{https://lucidtoolsource.gitlab.io/lucid/}.
\end{center}
% \lucid's source code is hosted at \url{https://gitlab.com/lucidtoolsource/lucid}.

We also provide a Python wrapper, called \pylucid, to facilitate the integration of the tool into existing workflows and effortlessly leverage well-established libraries such as NumPy~\cite{numpy} and SciPy~\cite{scipy}.
Moreover, \pylucid can be extensively configured in a variety of ways (e.g., Python scripts, YAML files, GUI), making it the recommended way to operate \lucid.
Hence, for the rest of the paper, we will focus on \pylucid when describing the user interfaces.
Further information about \pylucid are deferred to the supplementary material.


\paragraph{Configuration}
% The system and the specification \lucid will check against shall be provided as a configuration to the program.
To certify safety of a system, \lucid accepts a configuration comprising data from the system and the safety specification.
We suggest defining it as a \yaml or equivalent \json file.
%\footnote[4]{The Json schema for \yaml and \json files can be found at \url{https://lucidtoolsource.gitlab.io/lucid/configuration_schema.json}}
% \OS{I would personally avoid the word "scenario", as it could be misunderstood with an actual run of the system.}
If more flexibility is needed, a Python script generating a configuration can be used instead,
with the only requirement being that it must contain a function named \lstinline|scenario_config| returning a \texttt{Configuration} object.
Regardless of their format, configuration files can be loaded with the command \lstinline|pylucid <config file>| to start the tool's verification pipeline.
The same configuration values can also be given directly as command line arguments.
\pylucid also provides a browser-based \gls{gui} to aid in the configuration and execution of the tool.

In its current form, \lucid implements all the functionalities needed to certify safety of a closed-loop system with a given control policy $\pi$.
Thus the action $a$ in the previous section is replaced with the policy $\pi(x)$.
Future releases will expand this core functionality to
safe controller synthesis as well.
Due to its modular architecture, \lucid can be easily extended in multiple directions, as outlined in Section~\ref{sec:conclusion_and_future_extensions}.

\subsection{Configuring and Running the Tool}
% \todo{Map the theoretical modules from Section~\ref{sec:theory} to components/modules of the code.}

\lucid is built with a modular architecture where each component is responsible for a specific task in the certification process.
Moreover, since they extend from a common interface, they can be easily replaced or extended.
An overview of the core components and how they interact is shown in Figure~\ref{fig:architecture}.
Classes and interfaces available in \pylucid are written in \texttt{monospace font}.

To make the explanation more intuitive, we will use as a running example the one-dimensional linear system
\begin{equation}
    \label{eq:linear-system}
    \S\colon%\begin{bmatrix}
    \quad
    {x}_{t+1}
    %\end{bmatrix}
    = %\begin{bmatrix}
    0.5
        %\end{bmatrix}
        %\begin{bmatrix}
        {x}_{t}
    %\end{bmatrix} 
    + w_t,
    \quad w_t \sim \mathcal{N}(\cdotx | 0, 0.01),
\end{equation}
% where $x_t$ is the state at time $t$ and $w_t$ is Gaussian noise.
% Given the
with
state space $\X = [-1, 1]$, initial set $\X_0 = [-0.5, 0.5]$, and unsafe regions $\X_U = [-1, -0.9] \cup [0.9, 1]$.
While going over each component of \lucid, we will show how they can be configured within a \yaml configuration file to capture the system~\eqref{eq:linear-system} and its safety specification.

\paragraph{Dataset}
% \todo{Should this be considered a component? It is more about the data loader} \Sadegh{This is good to have as a separate component.}
Due to its data-driven nature, all that \lucid needs to operate, is a set of sampled transitions $\data_N$ from the system, as shown in \eqref{eq:data}, and the safety specification itself.
% Data can be obtained by defining a function that takes as input the current state and control input and returns the next state, adding i.i.d. noise sampled from an appropriate distribution, or it may be collected from an external simulator or real-world system.
The samples can be specified directly in the configuration file, divided into $x^1,\ldots,x^N$ and $x^1_+,\ldots,x^N_+$:
\begin{lstlisting}[language=yaml,backgroundcolor=\color{ipython_bg}]
x_samples: [[5.930e-01], ..., [-1.937e-01]]
xp_samples: [[3.015e-01], ..., [-1.018e-01]]
\end{lstlisting}
Alternatively, the samples can either be generated by the Python configuration script, or be stored in a separate file.
% Refer to the documentation for more details.
\begin{comment}
\begin{lstlisting}[language=iPython]
    def scenario_config():
    # ...
    # Sampling
    f = lambda x: x / 2
    c.x_samples = c.X_bounds.sample(1000)
    c.xp_samples = f(c.x_samples) + np.random.normal(scale=0.01)
    # ...
    return c
\end{lstlisting}
\end{comment}

\input{figures/architecture}

\paragraph{\Circled{1}~Safety Specification}
\lucid understands sets $\X,\X_0,\X_U$ specified as \texttt{RectSet}, \texttt{SphereSet}, or \texttt{Multiset} objects (collections of sets from the first two categories).
% Sets are mainly used to indicate the bounds of the whole state space $\X$, the initial set $\X_0$ and the unsafe set $\X_U$ in the context of defining the safety specification to verify.
They can be included in the configuration as shown below:
\begin{lstlisting}[language=yaml,backgroundcolor=\color{ipython_bg}]
X_bounds: "RectSet([-1], [1])"
X_init: "RectSet([-0.5], [0.5])"
X_unsafe: 
  - "RectSet([-1], [-0.9])"
  - "RectSet([0.9], [1])"
\end{lstlisting}
% \OS{So is the \lstinline{X_unsafe} here a Multiset?}

\paragraph{\mbox{\Circled{2}\hspace{.4em}Estimator}}
The core of \lucid is its \estimator, namely the \texttt{KernelRidgeRegressor}, which uses the \texttt{GaussianKernel} to learn the underlying system dynamics by estimating the \gls{cme} from the samples.
Its predictions of the expected next state $x_+$ are used
% of all the lattice points we need from $\X$, necessary 
to determine the constraints for the \gls{cbc} \gls{lp}.
% The \hp of the \texttt{KernelRidgeRegressor} can be set explicitly in the configuration file:
% We provide the \hp values for the kernel, namely the bandwidth $\sigma_f = 1$, the length scale $\sigma_l = 1.75555556$, values obtained from a previous tuning process, 
% and the regularization constant $\lambda = 0.00001$.
The setup is configured as follows:
\begin{lstlisting}[language=yaml,backgroundcolor=\color{ipython_bg}]
kernel: "GaussianKernel"
estimator: "KernelRidgeRegressor"
\end{lstlisting}


\paragraph{\Circled{3}~Tuner}
Being parameter-free approaches, kernel methods do not require the expensive learning processes of other machine learning methods, such as neural networks.
However, they still depend on a number of \hp, such as the kernel bandwidth $\sigma_f$, the lengthscale $\sigma_l$, and the regularization constant $\lambda$ (see \eqref{eq:gaussian_kernel}--\eqref{eq:empiricalEstimate}).
Changes in their values can have a significant impact on the \estimator's efficiency and accuracy.
The process of finding good values for these \hp is known as \emph{hyperparameter tuning}, with the optimal parameters being problem dependent.
\lucid provides a set of utilities, which specialize the \texttt{Tuner} interface, to aid in this task:
\begin{itemize}
    \item \texttt{MedianHeuristicTuner} uses the \emph{median heuristic} \cite{garreau2018largesampleanalysismedian} to produce rule-of-thumb-type estimates for the \hp $\sigma_f$ and $\sigma_l$ of, e.g., the Gaussian kernel \eqref{eq:gaussian_kernel}, via closed-form expressions.
          It can be useful as a starting point for subsequent improvements.
    \item \texttt{LbfgsTuner} finds the \hp that maximize the \emph{log marginal likelihood}~\cite{rasmussen2006gaussian}, defined as
          \begin{equation*}
              \begin{split}
                   & \log p(X^+_N | X_N,\theta) =  - \frac{1}{2}X^+_N{}\T (K_{X}^N + N\lambda I_N)^{-1} X^+_N \\
                   & \hspace{40pt} -\frac{1}{2}\log |K_{X}^N + N\lambda I_N| - \frac{N}{2}\log(2\pi),
              \end{split}
          \end{equation*}
          where $\theta:=(\sigma_f,\sigma_l,\lambda)$ is the hyperparameter being optimized.
          We use the L-BFGS or L-BFGS-B quasi-Newton optimization algorithms~\cite{book:nonlinear-programming},
          implemented in the \texttt{LBFGS++} library~\cite{LBFGSpp}.
          %          \footnote{https://github.com/yixuan/LBFGSpp}
    \item \texttt{GridSearchTuner} implements the grid search method, exploring the space of possible \hp values to find the one that maximizes the \estimator's $R^2$ score, defined as $$R^2 = 1 - \textstyle\left(\sum_{i=1}^M (y_i - \hat{y}_i)^2/\sum_{i=1}^M (y_i - \bar{y})^2\right),$$ with $\hat{y}_i$ and $\bar{y}:=(\sum_{i=1}^N y_i)/N$ being the $i^{\text{th}}$ predicted and mean observed outputs, respectively.
\end{itemize}
Tuners open a wide range of possibilities to the user.
We recommend using them within a Python script generating a configuration to automate the tuning process before finalizing the \estimator's hyperparameter values:
\begin{lstlisting}[language=iPython,style=nonumbers]
def scenario_config(c: Configuration):
    t = LbfgsTuner(lb=[1e-5], ub=[1e5])
    c.estimator = KernelRidgeRegressor()
    c.estimator.fit(c.x_samples, c.xp_samples, tuner=t)
    return c
\end{lstlisting}
In this example, we set the \hp explicitly:
\begin{lstlisting}[language=yaml,backgroundcolor=\color{ipython_bg}]
sigma_f: 15
sigma_l: [1.2]
lambda: 1.0e-5
\end{lstlisting}

\begin{comment}
A cheap starting point when working with a Gaussian kernel as in~\eqref{eq:gaussian_kernel} is to use the \emph{median heuristic} \cite{garreau2018largesampleanalysismedian}, implemented in the \texttt{MedianHeuristicTuner}.
\OS{We can cut the following to save space. This is standard knowledge:}
{\color{gray}
    We choose $\sigma_l^2$ to be the median of the squared distances between all pairs of points in the training set, i.e.,
    \begin{equation}
        \sigma_l^2 = \mathrm{median}_{1 \leq i < j \leq N}\left(\|\hat{x}_i-\hat{x}_j\|^2\right),
    \end{equation}
    where $\hat{x}_1,\ldots,\hat{x}_N$ are the $N$ training samples.
    If $N(N-1)/2$ is even, the median is the average of the two middle values in the sorted list of distances.}
The \texttt{GridSearchTuner}, on the other hand, implements the grid search method, exploring the space of possible \hp values to find the one that yields the best \estimator $R^2$ score,
defined as $R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}$.
This approach is simple to implement and can be very effective for small problems, but it can become computationally expensive for larger ones.
Borrowing a technique from Gaussian processes, we can also optimize the \hp to maximize the \emph{log marginal likelihood}, defined as
\begin{equation}
    \begin{split}
         & \log p(X^+_N | X_N,\theta) =  - \frac{1}{2}X^+_N{}\T (K_{X}^N + N\lambda I_N)^{-1} X^+_N \\
         & \hspace{40pt} -\frac{1}{2}\log |K_{X}^N + N\lambda I_N| - \frac{N}{2}\log(2\pi),
    \end{split}
\end{equation}
where $\theta:=(\sigma_f,\sigma_l, M)$ \OS{Which are implemented?} contains all learnable hyperparameters (of the kernel).
To do so, the \texttt{LbfgsTuner} uses the L-BFGS or L-BFGS-B quasi-Newton optimization algorithms~\cite{book:nonlinear-programming},
implemented in the \texttt{LBFGS++} library\footnote{https://github.com/yixuan/LBFGSpp}.
\end{comment}

\paragraph{\Circled{4}~Feature Map}
We exploit the spectral kernel expansion in \eqref{eq:sqexp_kernel_fourier} by building an explicit approximated feature map, composed of a linear sum of trigonometric functions with increasing frequencies (see Figure~\ref{fig:spectral_barrier}).
The underlying kernel expansion, visualized in Figure~\ref{fig:spectral_measure_abstraction}, can be controlled to trade-off efficiency and accuracy/conserativeness.
% The full expansion of the kernel would need a sum of infinitely many terms with diminishing impact on the overall result, which we truncate arbitrarily short, a decision based on the tradeoff we are looking for between efficiency and accuracy.
After being defined with the same \hp $(\sigma_f,\sigma_l)$ that characterize the \texttt{KernelRidgeRegressor}, the function can be used to map any point from $\X$ to the \gls{rkhs} associated with the kernel.
\lucid currently implements three different ways for partitioning the spectral measure in Figure~\ref{fig:spectral_measure_abstraction}:
\begin{itemize}
    \item \texttt{LinearTruncatedFourierFeatureMap} defines equally spaced frequency bands on the interval $[-3\sigma_l,3\sigma_l]$, capturing 99.73\% of the spectral measure. This partitioning is shown in Figure~\ref{fig:spectral_measure_abstraction}.
    \item \texttt{LogTruncatedFourierFeatureMap} defines logarithmically spaced frequency bands on the interval $[-3\sigma_l,3\sigma_l]$, with each frequency band carrying an equal share of the spectral measure.
    \item \texttt{ConstantTruncatedFourierFeatureMap} defines frequency bands with a fixed size.
\end{itemize}
Different feature map implementations may yield different results.
In our example, we truncate the Fourier expansion to $5$ frequency bands, including the constant term, and use $704$ lattice points to build the constraints of the optimization problem, to avoid it being too conservative.
\begin{lstlisting}[language=yaml,backgroundcolor=\color{ipython_bg}]
feature_map: 
    "LinearTruncatedFourierFeatureMap"
num_frequencies: 5
num_oversample: 704
\end{lstlisting}

\paragraph{\Circled{5} Solver}
% \todo{Barriers should probably be an explicit class in \lucid that hides the optimization implementation}
Based on the components discussed above, \lucid generates and solves a finite-constraint \gls{lp} as outlined in Section~\ref{sec:ddbarriers}.
% If successful, the output is a \glspl{cbc} and a quantified lower bound on the true safety probability $P_{safe}(\S^\pi)$.
%
% When verifying the safety of a system, \lucid will search for a \gls{cbc} $\B$ that satisfies the conditions in Definition~\ref{def:cbc}.
If successful, \lucid returns a \gls{cbc}, a quantified lower bound on the true safety probability $P_{safe}(\S^\pi)$, as well as the corresponding constants $\eta$, $\gamma$, and $c$.
% More precisely, we follow the steps outlined in Figure~\ref{fig:steps}.

Generating the \gls{lp}, specifically choosing an appropriate lattice density, involves trading off efficiency versus conservativeness, with any sampling density beyond the Nyquist frequency yielding a valid relaxation.
In the benchmarks in Section~\ref{sec:experiments}, we showcase the relation numerically.
% (see Section~\ref{sec:ddbarriers})
%
% While the theory gives us a precise indication on how to construct the \gls{lp} which yields a correct-by-design solution, in practice the constraints may be too conservative to find a valid solution.
% To mitigate this, the user can either increase the number of lattice points $Q$, worsening the computational efficiency,
% or adjust the coefficient $C_{\hat{N}}$, improving performance at the cost of loosening the guarantees.
For solving the \gls{lp}, \lucid can interface with different linear optimizers: \gurobi, \alglib, or \highs. Here, we use \gurobi:
\begin{lstlisting}[language=yaml,backgroundcolor=\color{ipython_bg}]
optimiser: "GurobiOptimiser"
\end{lstlisting}

\paragraph{Running the Tool}
We have configured all the components needed to run \lucid.
Putting it all together in a single configuration file \texttt{config.yaml} we can run the tool with the command \lstinline|pylucid config.yaml --plot|.
\lucid will parse the configuration, synthesize a barrier certificate and plot the result.
For the running example, we obtain the barrier certificate shown in Figure~\ref{fig:example} for a time horizon of $T = 15$ certifying safety of the system in \eqref{eq:linear-system} with a probability of at least $95.12\%$ ($\eta = 0.03, \gamma=1.0, c = 0.001$). %\|B\| = 1.72

\begin{figure}
    \centering
    \begin{adjustbox}{max width=\columnwidth}
        \input{figures/example.pgf}
    \end{adjustbox}
    %\resizebox{\columnwidth}{!}{\import{figures}{example.pgf}}
    \caption{Barrier certificate for the example.
        The value of the barrier $\B$ is plotted against the state space $\X$.
        The green line represents the barrier value.
        The initial and unsafe sets are highlighted in blue and red respectively. % \Sadegh{Figure not very readable.}
    }
    \label{fig:example}
\end{figure}

\paragraph{Validation (optional)}
Albeit, if \lucid finds a barrier, this is already formally sound with respect to the data-driven estimator,
if the system dynamics are known, \lucid provides the option to formally verify the correctness of the barrier using the \dreal SMT solver~\cite{dreal}.

% The resulting barrier can then be checked with the verifier, provided we have access to the system's model, to ensure its correctness.

Since we know the expected behavior of the example system $\S$ from~\eqref{eq:linear-system}, we can run the formal validation by adding the following two lines to the configuration:
\begin{lstlisting}[language=yaml,backgroundcolor=\color{ipython_bg}]
system_dynamics: ["x1 / 2"]
verify: true
\end{lstlisting}
Doing so confirms that $\B$ is indeed a valid \gls{cbc}.

\begin{comment}
\subsection{Functionality}

Having parsed and validated the scenario configuration, \lucid will begin the main pipeline to produce the expected result.
If a set of sample transitions has not been provided by the user, \lucid will try to produce it itself by randomly sampling $\X$ to which apply the system dynamics function.
The samples will be used to fit and optionally tune the \estimator so that, given an input state, it will accurately predict the value of the feature map applied to the next state.
\lucid will then store a lattice of points from each of the set provided to which apply the feature map.
These values will be used to define the constraints of the \gls{lp} that the chosen optimizer will solve.

\Sadegh{In the following figure, when you say collect samples, it should be the dataset mentioned earlier in the paper. You also need the next state.}

\section{Example Use Case}

\paragraph{Configuration via Python}
We can also define the same exact configuration in Python, allowing for more flexibility and programmatic control.
\lstinputlisting[language=iPython,caption={Python configuration for the example.}]{code/example.py}
Note that in this case we are building the \estimator ourself instead of just providing the type of \estimator to use.
This gives us absolute control over how the \estimator is tuned before being used.
\end{comment}

\section{Experimental Evaluation}
\label{sec:experiments}

To ascertain the performance of \lucid, we conduct a series of experiments on a Windows 10 machine with an AMD Ryzen 9 5950X 16-Core Processor @ 3.40 GHz, NVIDIA GeForce RTX 3090 GPU, and 64 GB of RAM.
All runs had the random seed set to $42$ to ensure reproducibility.

We adapt the \barrII and \barrIII benchmarks from \cite{abate2021fossil}.
Both are two-dimensional highly nonlinear systems to which we add the stochastic noise $w_t\sim\mathcal{N}(\cdotx\vert 0,0.01I_2)$.
Given $N=1000$ samples, initial set $\X_0$, and unsafe set $\X_U$, we synthesize a barrier $\B$ that guarantees trajectories starting in $\X_0$ do not enter $\X_U$ within $T=5$ time steps.
Note that we can only certify safety w.r.t. the empirical distribution, i.e., the \gls{cme} constructed from the observed data.
\barrIII is particularly challenging due to the closeness between the initial set and the unsafe set.
The synthesized barriers are shown in Figures~\ref{fig:CSBarr2}--\ref{fig:CSBarr3}.
We also consider a scenario, we name \overtaking, where an autonomous vehicle controlled by a \gls{nn} is overtaking another vehicle.
The dynamics of the ego vehicle are given by Dubin's car model with an added noise vector $w$ where each component is drawn from a zero-mean Gaussian with standard deviation $0.01$, $0.01$, and $0.001$ respectively.
The steering wheel angle is supplied by the \gls{nn} controller and we travel at a fixed velocity.

Table~\ref{tbl:benchmarks_results} summarizes the results of all the experiments presented.
Further results and details on the dynamics, configurations, and \hp can be found in the supplementary material.

\begin{figure}[ht]
    \begin{adjustbox}{clip,trim=0cm 1.5cm 0cm 2cm,max width=\columnwidth}
        \input{figures/barrier2.pgf}
    \end{adjustbox}
    %\includegraphics[trim={5cm 3cm 2cm 4cm},clip,width=\linewidth]{barrier2.pgf}
    \caption{\gls{cbc} synthesized for the \barrII benchmark.}
    \label{fig:CSBarr2}
\end{figure}

\begin{figure}[ht]
    \begin{adjustbox}{clip,trim=0cm 2cm 0cm 2cm,max width=\columnwidth}
        \input{figures/barrier3.pgf}
    \end{adjustbox}
    %\resizebox{\columnwidth}{!}{\import{figures}{barrier3.pgf}}%
    \caption{\gls{cbc} synthesized for the \barrIII benchmark.}
    \label{fig:CSBarr3}
\end{figure}


\begin{table}[tb]
    \centering
    \begin{tabular}{cccccc}
        \toprule
                    & \textbf{T} & \textbf{Freq.} & \textbf{Lattice} & \textbf{Runtime} & \textbf{Safety} \\
                    &            &                & \textbf{Size}    & [mm:ss]          & \textbf{Prob.}  \\ % Units
        \midrule
        \linear     & 15         & 5              & $704$            & 00:02            & 95.12\%         \\
        \barrII     & 5          & 15             & $350^2$          & 10:01            & 56.87\%         \\
        \barrIII    & 5          & 15             & $800^2$          & 64:20            & 42.25\%         \\
        \overtaking & 5          & 5              & $99^3$           & 46:33            & 30.32\%         \\
        \bottomrule
    \end{tabular}
    \caption{Computational benchmarks.
        The lattice size indicates the number of points of the lattice as the resolution for each dimension raised to the number of dimensions.
        $T$ is the time horizon associated with the lower bound on the safety probability, displayed in the last column.
    }
    \label{tbl:benchmarks_results}
    \vspace{-0.5cm}
\end{table}

% Linear
% Success: 93.74%, c 0.0032813476604243406, eta 0.013421457997340467, lambda 1e-05, num_frequencies 5, num_oversample 704, oversample_factor 2.0, sigma_l 0.034, sigma_f 18.0, T 15
% Barrier3
% Success: 42.25%, c 0.11928419647787443, eta 0.5584972599863635, lambda 1e-08, num_frequencies 15, num_oversample 800, oversample_factor 1.0, sigma_l [2.99266 4.62946], sigma_f 1.0, T 5
% Overtaking
% Success: 30.32%, c 0.05031756638044985, eta 0.09681458793205037, lambda 1e-05, num_frequencies 5, num_oversample -1, oversample_factor 9.0, sigma_l [10.  7.  5.], sigma_f 7.0, T 5


% \begin{table}
%     \centering
%     \begin{tabular}{cccc}
%         \toprule
%         \textbf{Benchmark} & \textbf{Dim} & \textbf{Runtime} & \textbf{Safety Prob.} \\
%                            &              & [m:ss]           & [\%]                  \\ % Units
%         \midrule
%         Barr3              & 2            & X:XX             & XX                    \\
%         name1              & 3            & X:XX             & XX                    \\
%         name2              & 7            & X:XX             & XX                    \\
%         \bottomrule
%     \end{tabular}
%     \caption{Computational benchmarks: ...}
%     \label{tbl:benchmarks}
% \end{table}

%\Sadegh{Are we comparing with alternative approaches in the literature? Since we mention alternative tools and their capabilities in Table 1, this raises expectations that we provide a fair benchmarking and show how we are better than them.}

\section{Conclusion and Future Extensions}\label{sec:conclusion_and_future_extensions}
This paper introduces \lucid, a novel tool capable of quantifying safety guarantees for black-box systems with complex stochastic dynamics and deep learning components in the loop. As such, \lucid fills a gap currently unaddressed by preexisting tools. To achieve this, \lucid leverages Conditional Mean Embeddings (CME) to learn control barrier certificates from data, and recasts the problem into a tractable linear form via a spectral abstraction.

\lucid is built for extensibility. In its current form, it assumes access to full state measurements (cf.~\eqref{eq:data}), as is the case when working with simulated systems, and focuses on verification, i.e., when a policy is given.
Extending \lucid to partially observed settings and safe controller synthesis
% adds another level of complexity and 
is on our agenda and builds on top of the results presented in this paper.
\lucid derives barriers based on the empirical CME, which can be robustified against out-of-sample system behavior by tightening the barrier constraint (c) (cf.~Sections~\ref{sec:dd_estimation_via_cme}--\ref{sec:ddbarriers}).
The scalability of \lucid can be improved by performing sparse CME computations or specializing the estimator's \texttt{Kernel} to embed prior knowledge about the system dynamics.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{aaai25}

\input{appendix}

\newpage
% \def\isChecklistMainFile{\FALSE}
% \input{reproducibility checklist/ReproducibilityChecklist}

\end{document}
