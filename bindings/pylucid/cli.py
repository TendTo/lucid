import importlib
import json
import sys
from argparse import Action, ArgumentDefaultsHelpFormatter, ArgumentParser, Namespace
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np
import scipy.io
import yaml
from jsonschema import Draft202012Validator
from jsonschema.exceptions import ValidationError

from ._constants import *
from ._pylucid import *
from ._pylucid import __version__
from .parser import SetParser, SympyParser
from .util import assert_or_raise, raise_error

if TYPE_CHECKING:
    from typing import Any, Callable

    from ._pylucid import NMatrix, NVector


@dataclass
class Configuration(Namespace):
    """Configuration determining the scenario.

    Attributes:
        verbose: Verbosity level for logging. It goes from -1 (no logging) to 5 (TRACE)
        seed: Random seed for reproducibility. If < 0, no seeding is done
        input: Path to the configuration file. Can be a .py, .yaml or .json file
        plot: Whether to plot the solution using plotly
        verify: Whether to verify the barrier certificate using dReal
        problem_log_file: File to save the optimization problem in LP format. If empty, the problem will not be saved
        iis_log_file: File to save the irreducible infeasible set (IIS) in ILP format. If empty, the IIS will not be saved

        system_dynamics: Deterministic function that maps the state variable x to the next state variable x+
        X_bounds: Set that bounds the state space
        X_init: Set of initial states
        X_unsafe: Set of unsafe states

        x_samples: Matrix of row-vector input samples from the state space. If not provided, it will be generated from the X_bounds
        xp_samples: Matrix of row-vector samples for the next state x+. If not provided, it will be generated by applying the system dynamics to the x_samples
        f_xp_samples: Precomputed feature map application to the next state variable x+. If not provided, it will be generated by applying the appropriate feature map
        num_samples: Number of samples to use for training the estimator. Only used if the samples are not provided
        noise_scale: Scale of the gaussian noise added to the generated xp_samples. If 0, no noise is added. Only used if xp_samples are not provided

        lambda_: Regularization constant for the estimator
        sigma_f: Amplitude parameter for the kernel
        sigma_l: Variance parameter for the kernel, can be a single float (isotropic) or an array of floats (anisotropic)

        num_frequencies: Number of frequencies per dimension for the feature map. Includes the constant frequency (0)
        oversample_factor: Factor by which to oversample the frequency space with respect to the nyquist frequency (i.e., if set to 1 is the nyquist frequency). It is ignored if num_oversample is a positive number
        num_oversample: Number of lattice points for each dimension. Must be greater than the nyquist frequency. If negative, it is computed based on the oversample_factor
        gamma: Constant such that the barrier value over the unsafe set is at least gamma
        c_coefficient: coefficient that makes the optimization more (> 1) or less (< 1) conservative
        time_horizon: The number of time steps for which the barrier certificate is computed
        epsilon: Robustifying radius
        b_norm: Expected value of the barrier norm
        b_kappa: Coefficient

        estimator: Estimator class to use for regression
        kernel: Kernel class to use for the estimator
        feature_map: Feature map class to use for transformation or a callable that returns a feature map
        optimiser: Optimiser class to use for the optimization
        tuner: Tuner for the estimator, if any

        constant_lattice_points: Flag to indicate whether to use a constant number of lattice points. Deprecated
    """

    # Global generic configuration
    verbose: int = log.LOG_INFO
    seed: int = -1
    input: Path = field(default_factory=Path)
    plot: bool = False
    verify: bool = False
    problem_log_file: str = ""
    iis_log_file: str = ""

    # System dynamics and specification
    system_dynamics: "Callable[[NMatrix], NMatrix] | None" = None
    X_bounds: "Set | None" = None
    X_init: "Set | None" = None
    X_unsafe: "Set | None" = None

    # Transition samples
    x_samples: "NMatrix" = field(default_factory=lambda: np.empty((0, 0), dtype=np.float64))
    xp_samples: "NMatrix | Callable[[NMatrix], NMatrix]" = field(
        default_factory=lambda: np.empty((0, 0), dtype=np.float64)
    )
    f_xp_samples: "NMatrix | Callable[[NMatrix], NMatrix] | None" = None
    num_samples: int = 1000
    noise_scale: float = 0.01

    # Kernel regression parameters
    lambda_: float = 1e-6
    sigma_f: float = 1.0
    sigma_l: "NVector | float" = 1.0

    # Barrier certificate parameters
    num_frequencies: int = 10
    oversample_factor: float = 2.0
    num_oversample: int = -1
    gamma: float = 1.0
    c_coefficient: float = 1.0
    time_horizon: int = 5
    epsilon: float = 0
    b_norm: float = 1.0
    b_kappa: float = 1.0

    # Classes to use for the pipeline
    estimator: "type[Estimator]" = KernelRidgeRegressor
    kernel: "type[Kernel]" = GaussianKernel
    feature_map: "type[FeatureMap] | FeatureMap | Callable[[Estimator], FeatureMap]" = LinearTruncatedFourierFeatureMap
    optimiser: "type[Optimiser]" = GurobiOptimiser if GUROBI_BUILD and GurobiOptimiser is not None else SoplexOptimiser
    tuner: "Tuner | None" = None

    constant_lattice_points: bool = False

    def populate_samples(self):
        if len(self.x_samples) == 0:
            # If x_samples is not provided, sample it from the bounds
            self.x_samples = self.X_bounds.sample(self.num_samples)
        if len(self.xp_samples) == 0:
            assert_or_raise(
                self.system_dynamics is not None,
                "If no outputs are provided, 'system_dynamics' must be specified",
            )
            # If xp_samples is not provided, compute it using the system dynamics function
            # Noisy system dynamics
            f = lambda x: self.system_dynamics(x) + np.random.normal(scale=self.noise_scale)
            self.xp_samples = f(self.x_samples)

    def to_safe_dict(self) -> dict:
        config_dict = self.__dict__.copy()
        config_dict["system_dynamics"] = []
        for k, v in config_dict.items():
            if isinstance(v, np.ndarray):
                config_dict[k] = v.tolist()
            elif isinstance(v, type):
                config_dict[k] = v.__name__
            elif isinstance(v, Path):
                config_dict[k] = str(v)
            elif isinstance(v, Set):
                config_dict[k] = str(v)
            elif isinstance(v, (Estimator, Kernel, FeatureMap, Optimiser, GurobiOptimiser)):
                config_dict[k] = v.__class__.__name__
        return config_dict

    def to_yaml(self, path: "str | Path | None" = None) -> str:
        """Convert the configuration to a YAML string or save it to a file."""
        config_dict = self.to_safe_dict()
        yaml_str = yaml.safe_dump(config_dict, default_flow_style=False, sort_keys=False)
        if path is not None:
            with open(path, "w", encoding="utf-8") as f:
                f.write(yaml_str)
        return yaml_str

    def shallow_copy(self) -> "Configuration":
        """Create a shallow copy of the configuration."""
        return Configuration(**self.__dict__.copy())

    @classmethod
    def from_file(cls, path: "str | Path") -> "Configuration":
        """Load a configuration from a YAML file."""
        config = cls()
        action = ConfigAction(option_strings=None, dest="input")
        action(None, config, Path(path))
        return config

    @classmethod
    def from_dict(cls, config_dict: dict) -> "Configuration":
        """Load a configuration from a dictionary."""
        config = cls()
        ConfigAction.dict_to_configuration(config_dict, config)
        return config


class ConfigAction(Action):
    """Custom action to handle configuration files."""

    def __call__(self, parser, namespace: Configuration, values: Path, option_string=None):
        """Parse the configuration file and update the namespace."""
        assert isinstance(values, Path), "Input must be a Path object"
        assert_or_raise(values.exists(), f"Configuration file does not exist: {values}")

        setattr(namespace, self.dest, values)
        if values.suffix == ".py" or values == Path():
            # We don't need to load a config file, just set the input path as provided
            return

        suffixes = (".py", ".yaml", ".json", ".yml")
        assert_or_raise(
            values.suffix in suffixes, f"Unsupported file type: {values.suffix}. Supported types are {suffixes}"
        )

        # We won't need to use the path later, just store an empty Path object

        # Load the configuration file and the JSON schema
        with open(values, "r", encoding="utf-8") as f:
            config = json.load(f) if values.suffix == ".json" else yaml.safe_load(f)
        if not isinstance(config, dict):
            raise raise_error(f"Configuration file must contain a dictionary, got {type(config)} instead")
        # Validate the configuration dictionary against the schema
        self.validate(config, verbosity=namespace.verbose)

        # Convert the dictionary to configuration and update the namespace
        self.dict_to_configuration(config, namespace)

    def validate(self, config_dict: dict, verbosity: int = log.LOG_INFO):
        """Validate the configuration dictionary against the schema."""
        if sys.version_info < (3, 9):
            with importlib.resources.open_text("pylucid", "configuration_schema.json", encoding="utf-8") as schema_file:
                schema = json.load(schema_file)
        else:
            with importlib.resources.files("pylucid").joinpath("configuration_schema.json").open(
                "r", encoding="utf-8"
            ) as schema_file:
                schema = json.load(schema_file)

        try:
            Draft202012Validator(schema).validate(instance=config_dict)
        except ValidationError as e:
            raise raise_error(f"{e.message}", ValidationError) from (e if verbosity >= log.LOG_DEBUG else None)

    @classmethod
    def dict_to_configuration(cls, config_dict: dict, args: Configuration) -> Configuration:
        """
        Convert a dictionary parsed from a YAML or JSON file to a Configuration object.

        Args:
            config_dict: Dictionary parsed from a configuration file

        Returns:
            Configuration object with the configuration values
        """
        # Process basic parameters in order matching Configuration class definition
        # Global generic configuration
        args.input = Path()
        args.verbose = int(config_dict.get("verbose", args.verbose))
        args.seed = int(config_dict.get("seed", args.seed))
        args.plot = bool(config_dict.get("plot", args.plot))
        args.verify = bool(config_dict.get("verify", args.verify))
        args.problem_log_file = str(config_dict.get("problem_log_file", args.problem_log_file))
        args.iis_log_file = str(config_dict.get("iis_log_file", args.iis_log_file))

        # Transition samples
        args.num_samples = int(config_dict.get("num_samples", args.num_samples))
        args.noise_scale = float(config_dict.get("noise_scale", args.noise_scale))

        # Kernel regression parameters
        # Note: JSON uses "lambda", not "lambda_"
        args.lambda_ = float(config_dict.get("lambda", args.lambda_))
        args.sigma_f = float(config_dict.get("sigma_f", args.sigma_f))

        # Barrier certificate parameters
        args.num_frequencies = int(config_dict.get("num_frequencies", args.num_frequencies))
        args.oversample_factor = float(config_dict.get("oversample_factor", args.oversample_factor))
        args.num_oversample = int(config_dict.get("num_oversample", args.num_oversample))
        args.gamma = float(config_dict.get("gamma", args.gamma))
        args.c_coefficient = float(config_dict.get("c_coefficient", args.c_coefficient))
        args.time_horizon = int(config_dict.get("time_horizon", args.time_horizon))
        args.epsilon = float(config_dict.get("epsilon", args.epsilon))
        args.b_norm = float(config_dict.get("b_norm", args.b_norm))
        args.b_kappa = float(config_dict.get("b_kappa", args.b_kappa))

        # Process system dynamics first
        system_dynamics = config_dict.get("system_dynamics", args.system_dynamics)
        if isinstance(system_dynamics, list) and len(system_dynamics) > 0:
            SystemDynamicsAction(option_strings=None, dest="system_dynamics")(None, args, system_dynamics)

        # Process sets
        set_parser = None
        for set_name in ("X_bounds", "X_init", "X_unsafe"):
            set_value = config_dict.get(set_name, getattr(args, set_name))
            if set_value is None or isinstance(set_value, Set):
                setattr(args, set_name, set_value)
            elif isinstance(set_value, str):
                set_parser = set_parser or SetParser()
                setattr(args, set_name, set_parser.parse(config_dict[set_name]))
            else:
                set_value = cls.parse_set_from_config(config_dict[set_name])
                setattr(args, set_name, set_value)

        # Process transition samples
        NMatrixAction(option_strings=None, dest="x_samples")(None, args, config_dict.get("x_samples", args.x_samples))
        NMatrixAction(option_strings=None, dest="xp_samples")(
            None, args, config_dict.get("xp_samples", config_dict.get("f_xp_samples", args.xp_samples))
        )

        # Process kernel regression parameters
        FloatOrNVectorAction(option_strings=None, dest="sigma_l")(None, args, config_dict.get("sigma_l", args.sigma_l))

        # Process classes for the pipeline
        EstimatorAction(option_strings=None, dest="estimator")(None, args, config_dict.get("estimator", args.estimator))
        KernelAction(option_strings=None, dest="kernel")(None, args, config_dict.get("kernel", args.kernel))
        FeatureMapAction(option_strings=None, dest="feature_map")(
            None, args, config_dict.get("feature_map", args.feature_map)
        )
        OptimiserAction(option_strings=None, dest="optimiser")(None, args, config_dict.get("optimiser", args.optimiser))

    @classmethod
    def parse_set_from_config(cls, set_config: "dict | list | str") -> "Set":
        """Helper function to parse set objects from dictionary/list representation"""
        if isinstance(set_config, str):
            set_parser = SetParser()
            return set_parser.parse(set_config)
        if not isinstance(set_config, dict):
            if len(set_config) == 0:
                raise raise_error("Set configuration cannot be empty")
            if len(set_config) == 1 and isinstance(set_config[0], dict):
                return cls.parse_set_from_config(set_config[0])
            return MultiSet(*tuple(cls.parse_set_from_config(rect_item) for rect_item in set_config))
        assert isinstance(set_config, dict), "Set configuration must be a dictionary or a list of dictionaries"
        if "RectSet" in set_config:
            rect_data = set_config["RectSet"]
            return (
                RectSet(rect_data["lower"], rect_data["upper"]) if isinstance(rect_data, dict) else RectSet(rect_data)
            )
        if "SphereSet" in set_config:
            sphere_data = set_config["SphereSet"]
            return SphereSet(sphere_data["center"], sphere_data["radius"])
        raise raise_error(f"Unsupported set type in dictionary: {set_config}")


class FloatOrNVectorAction(Action):
    def __init__(self, **kwargs):
        super().__init__(nargs="+", **kwargs)

    def __call__(self, parser, namespace, values: "float | list[float]", option_string=None):
        if isinstance(values, (float, int)):
            values = [values]
        setattr(namespace, self.dest, np.array(values, dtype=np.float64) if len(values) > 1 else float(values[0]))


class SystemDynamicsAction(Action):
    def __init__(self, **kwargs):
        super().__init__(nargs="+", **kwargs)

    def __call__(self, parser, namespace, values: "list[str]", option_string=None):
        sym_parser = SympyParser()
        functions = sym_parser.parse_to_lambda(values)

        def system_dynamics_func(x: "NMatrix") -> "NMatrix":
            """Dynamic function that takes a state vector and returns the next state."""
            assert x.ndim == 2, "Input must be a 2D array with shape (n_samples, n_features)"
            cols = {f"x{i + 1}": x[:, i] for i in range(x.shape[1])}
            f_cols = [f(**cols) for f in functions]
            rows = max(len(f_col) for f_col in f_cols if isinstance(f_col, np.ndarray))
            # Broadcast scalar values to match the number of rows of the output
            for i, f_col in enumerate(f_cols):
                if np.isscalar(f_col):
                    f_cols[i] = np.full((rows,), f_col, dtype=np.float64)
            return np.column_stack(f_cols)

        setattr(namespace, self.dest, system_dynamics_func if functions else None)


def type_valid_path(path_str: str) -> Path:
    if not path_str:
        return Path()  # Allow empty input for default behavior
    path = Path(path_str)
    if not path.exists():
        raise raise_error(f"Path does not exist: {path_str}")
    return path


def type_set(set_str: str) -> "Set":
    """Convert a string representation of a function into a callable."""
    return SetParser().parse(set_str)


class EstimatorAction(Action):
    def __call__(self, parser, namespace, values: "str | type[Estimator]", option_string=None):
        if isinstance(values, type) and issubclass(values, Estimator):
            return setattr(namespace, self.dest, values)
        if values == "KernelRidgeRegressor":
            return setattr(namespace, self.dest, KernelRidgeRegressor)
        raise raise_error(f"Unsupported estimator type: {values}")


class KernelAction(Action):
    def __call__(self, parser, namespace, values: "str | type[Kernel]", option_string=None):
        if isinstance(values, type) and issubclass(values, Kernel):
            return setattr(namespace, self.dest, values)
        if values == "GaussianKernel":
            return setattr(namespace, self.dest, GaussianKernel)
        raise raise_error(f"Unsupported kernel type: {values}")


class FeatureMapAction(Action):
    def __call__(self, parser, namespace, values: "str | type[FeatureMap]", option_string=None):
        if isinstance(values, type) and issubclass(values, FeatureMap):
            return setattr(namespace, self.dest, values)
        if values == "LogTruncatedFourierFeatureMap":
            return setattr(namespace, self.dest, LogTruncatedFourierFeatureMap)
        if values == "ConstantTruncatedFourierFeatureMap":
            return setattr(namespace, self.dest, ConstantTruncatedFourierFeatureMap)
        if values == "LinearTruncatedFourierFeatureMap":
            return setattr(namespace, self.dest, LinearTruncatedFourierFeatureMap)
        raise raise_error(f"Unsupported feature map type: {values}")


class OptimiserAction(Action):
    def __call__(self, parser, namespace, values: "str | type[Optimiser]", option_string=None):
        if isinstance(values, type) and issubclass(values, Optimiser):
            return setattr(namespace, self.dest, values)
        if values == "GurobiOptimiser":
            return setattr(namespace, self.dest, GurobiOptimiser)
        if values == "AlglibOptimiser":
            return setattr(namespace, self.dest, AlglibOptimiser)
        if values == "HighsOptimiser":
            return setattr(namespace, self.dest, HighsOptimiser)
        if values == "SoplexOptimiser":
            return setattr(namespace, self.dest, SoplexOptimiser)
        raise raise_error(f"Unsupported optimiser type: {values}")


class NMatrixAction(Action):
    def __call__(self, parser, namespace, values: "str | type[NMatrix]", option_string=None):
        if values is None:
            return setattr(namespace, self.dest, np.empty((0, 0), dtype=np.float64))
        if isinstance(values, np.ndarray):
            return setattr(namespace, self.dest, values)
        if isinstance(values, list):
            return setattr(namespace, self.dest, np.array(values, dtype=np.float64))
        suffixes = (".npy", ".npz", ".csv", ".mat")
        path_to_file = Path(values)
        if path_to_file.suffix in suffixes:
            assert_or_raise(path_to_file.exists(), f"File does not exist: {values}")
            if path_to_file.suffix in (".npy", ".npz"):
                data = np.load(path_to_file, allow_pickle=True)
                if isinstance(data, np.lib.npyio.NpzFile):
                    # If it's a .npz file, we need to extract the first array
                    data = next(iter(data.values()))
            elif path_to_file.suffix == ".csv":
                with open(path_to_file, "rb") as f:
                    # Load CSV data, assuming it is a 2D array
                    lines = f.readlines()
                data = np.genfromtxt(lines, delimiter=",", dtype=np.float64).reshape(len(lines), -1)
                data = data[~np.isnan(data).any(axis=1)]
            elif path_to_file.suffix == ".mat":
                mat_data: "dict[str, Any]" = scipy.io.loadmat(path_to_file)
                for k, v in mat_data.items():
                    if k.startswith("__"):
                        continue
                    data = v
        else:
            try:
                data = np.array(json.loads(values), dtype=np.float64)
            except json.JSONDecodeError:
                raise raise_error(f"Invalid JSON string: {values}")
        assert_or_raise(isinstance(data, np.ndarray), f"Expected a numpy array, got {type(data)}")
        assert_or_raise(len(data) > 0, f"CSV file is empty: {values}")
        assert_or_raise(data.ndim == 2, f"Data must be a 2D array, got {data.ndim}D array instead")
        return setattr(namespace, self.dest, data)


class MultiNMatrixAction(Action):
    def __call__(self, parser, namespace: Configuration, values: "str", option_string=None):
        assert_or_raise(isinstance(values, str), "Input must be a string representing a file path and parsing info")
        if ":" in values:
            path_to_file, info = values.split(":")
            path_to_file = Path(path_to_file)
        else:
            path_to_file, info = Path(values), ""
        assert_or_raise(path_to_file.exists(), f"File does not exist: {values}")
        if path_to_file.suffix == ".npz":
            raise NotImplementedError("MultiNMatrixAction does not support .npz files with multiple arrays. ")
        elif path_to_file.suffix == ".csv":
            cols = int(info) if info.isdigit() else (namespace.X_bounds.dimension if namespace.X_bounds else 0)
            assert_or_raise(cols > 0, f"Invalid number of columns specified: {cols}")
            with open(path_to_file, "rb") as f:
                # Load CSV data, assuming it is a 2D array
                lines = f.readlines()
            data = np.genfromtxt(lines, delimiter=",", dtype=np.float64).reshape(len(lines), -1)
            data = data[~np.isnan(data).any(axis=1)]
            x_samples, xp_samples = data[:, :cols], data[:, cols:]
        elif path_to_file.suffix == ".mat":
            if "," in info:
                x_key, xp_key = info.split(",")
            else:
                x_key, xp_key = "x_samples", "xp_samples"
            mat_data: "dict[str, Any]" = scipy.io.loadmat(path_to_file)
            x_samples, xp_samples = mat_data.get(x_key), mat_data.get(xp_key)
        for data in (x_samples, xp_samples):
            assert_or_raise(isinstance(data, np.ndarray), f"Expected a numpy array, got {type(data)}")
            assert_or_raise(len(data) > 0, f"CSV file is empty: {values}")
            assert_or_raise(data.ndim == 2, f"Data must be a 2D array, got {data.ndim}D array instead")
        namespace.x_samples = x_samples
        namespace.xp_samples = xp_samples


def arg_parser() -> "ArgumentParser":
    config = Configuration()
    parser = ArgumentParser(prog="pylucid", description=__doc__, formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")
    parser.add_argument(
        "input",
        help="path to the configuration file. Can be a .py, .yaml or .json file. "
        "Command line parameter will override the values in the file. "
        "Python configuration files offer the most flexibility",
        nargs="?",
        action=ConfigAction,
        default=Path(),
        type=type_valid_path,
    )
    parser.add_argument(
        "--samples",
        help="path to the samples file followed by additional information on how to parse it. "
        "Can be a .npz, .csv or .mat file. The additional information follows a colon (:). "
        "For .mat files, you must specify the variable names to extract, e.g., '/path/to/samples.mat:X,XP'. "
        "For .csv files, you must specify the input dimension, e.g., '/path/to/samples.csv:3'. ",
        nargs="?",
        action=MultiNMatrixAction,
        type=str,
    )

    # Global generic configuration arguments
    parser.add_argument(
        "-v",
        "--verbose",
        type=int,
        help=f"verbosity level for logging."
        f"{log.LOG_NONE}: no output, "
        f"{log.LOG_CRITICAL}: critical, "
        f"{log.LOG_ERROR}: errors, "
        f"{log.LOG_WARN}: warning, "
        f"{log.LOG_INFO}: info, "
        f"{log.LOG_DEBUG}: debug, "
        f"{log.LOG_TRACE}: trace",
        choices=[
            log.LOG_NONE,
            log.LOG_CRITICAL,
            log.LOG_ERROR,
            log.LOG_WARN,
            log.LOG_INFO,
            log.LOG_DEBUG,
            log.LOG_TRACE,
        ],
        default=config.verbose,
    )
    parser.add_argument(
        "-s",
        "--seed",
        type=int,
        default=config.seed,
        help="random seed for reproducibility. If < 0, no seeding is done",
    )
    parser.add_argument(
        "--plot",
        action="store_true",
        help="whether to plot the solution using plotly",
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="whether to verify the barrier certificate using dReal",
    )
    parser.add_argument(
        "--problem_log_file",
        type=str,
        default=config.problem_log_file,
        help="file to save the optimization problem in LP format. If empty, the problem will not be saved",
    )
    parser.add_argument(
        "--iis_log_file",
        type=str,
        default=config.iis_log_file,
        help="file to save the irreducible infeasible set (IIS) in ILP format. If empty, the IIS will not be saved",
    )

    # System dynamics and specification arguments
    parser.add_argument(
        "--system_dynamics",
        type=str,
        default=config.system_dynamics,
        action=SystemDynamicsAction,
        help="deterministic function that maps the state variable x to the next state variable x+. "
        "Specify a function for each dimension of the output space. "
        "Variables `x1`, `x2`, ..., `xn` stand for the n-dimensional input state space components. "
        "All components of the input state space must be present in the function. "
        "For example, `--system_dynamics 'x1**2 + x2 / 2' '2 * x1 + sin(-x2)' 'cos(x1)'` "
        "will produce a function that takes a 2D input (x1, x2) and returns a 3D output (y1, y2, y3)",
    )
    parser.add_argument(
        "--X_bounds",
        type=type_set,
        default=config.X_bounds,
        help="set that bounds the state space. For example, `--X_bounds 'RectSet([-3, -2], [2.5, 1])'`",
    )
    parser.add_argument(
        "--X_init",
        type=type_set,
        default=config.X_init,
        help="set of initial states. "
        "For example, `--X_init 'MultiSet(RectSet([1, -0.5], [2, 0.5]), RectSet([-1.8, -0.1], [-1.2, 0.1]))'`",
    )
    parser.add_argument(
        "--X_unsafe",
        type=type_set,
        default=config.X_unsafe,
        help="set of unsafe states. "
        "For example, `--X_unsafe 'MultiSet(RectSet([0.4, 0.1], [0.6, 0.5]), RectSet([0.4, 0.1], [0.8, 0.3]))'`",
    )

    # Transition samples arguments
    parser.add_argument(
        "-i",
        "--x-samples",
        help="row-vector input samples from the state space. If not provided, it will be generated from the X_bounds. "
        "Can be a json-like 2D array or path to a .npy, .npz or .csv file",
        action=NMatrixAction,
        type=str,
    )
    parser.add_argument(
        "-o",
        "--xp-samples",
        help="row-vector samples for the next state variable x+. If not provided, it will be generated by applying the system dynamics to the x_samples. "
        "Can be a json-like 2D array or path to a .npy, .npz or .csv file",
        action=NMatrixAction,
        type=str,
    )
    parser.add_argument(
        "-N",
        "--num_samples",
        type=int,
        default=config.num_samples,
        help="number of samples to use for training the estimator. Only used if the samples are not provided",
    )
    parser.add_argument(
        "--noise_scale",
        type=float,
        default=config.noise_scale,
        help="scale of the gaussian noise added to the generated xp_samples. If 0, no noise is added. Only used if xp_samples are not provided",
    )

    # Kernel regression parameters arguments
    parser.add_argument(
        "-l",
        "--lambda",
        dest="lambda_",
        type=float,
        default=config.lambda_,
        help="regularization constant for the estimator",
    )
    parser.add_argument(
        "--sigma_f",
        type=float,
        default=1.0,
        help="amplitude parameter for the kernel",
    )
    parser.add_argument(
        "--sigma_l",
        default=config.sigma_l,
        type=float,
        action=FloatOrNVectorAction,
        help="variance parameter for the kernel, can be a single float (isotropic) or an array of floats (anisotropic)",
    )

    # Barrier certificate parameters arguments
    parser.add_argument(
        "-f",
        "--num_frequencies",
        type=int,
        default=config.num_frequencies,
        help="number of frequencies per dimension for the feature map. Includes the constant frequency (0)",
    )
    parser.add_argument(
        "--oversample_factor",
        type=float,
        default=config.oversample_factor,
        help="factor by which to oversample the frequency space with respect to the nyquist frequency (i.e., if set to 1 is the nyquist frequency). It is ignored if num_oversample is a positive number",
    )
    parser.add_argument(
        "--num_oversample",
        type=int,
        default=config.num_oversample,
        help="number of lattice points for each dimension. Must be greater than the nyquist frequency. If negative, it is computed based on the oversample_factor",
    )
    parser.add_argument(
        "-g",
        "--gamma",
        type=float,
        default=config.gamma,
        help="constant such that the barrier value over the unsafe set is at least gamma",
    )
    parser.add_argument(
        "-c",
        "--c_coefficient",
        type=float,
        default=config.c_coefficient,
        help="coefficient that makes the optimization more (> 1) or less (< 1) conservative",
    )
    parser.add_argument(
        "-T",
        "--time_horizon",
        type=int,
        default=config.time_horizon,
        help="the number of time steps for which the barrier certificate is computed",
    )
    parser.add_argument(
        "--epsilon",
        default=config.epsilon,
        type=float,
        help="robustifying radius",
    )
    parser.add_argument(
        "--b_norm",
        type=float,
        default=config.b_norm,
        help="expected value of the barrier norm",
    )
    parser.add_argument(
        "--b_kappa",
        type=float,
        default=config.b_kappa,
        help="coefficient",
    )

    # Classes to use for the pipeline arguments
    parser.add_argument(
        "--estimator",
        action=EstimatorAction,
        default=config.estimator,
        choices=["KernelRidgeRegressor"],
        help="estimator class to use for regression",
    )
    parser.add_argument(
        "--kernel",
        action=KernelAction,
        default=config.kernel,
        choices=["GaussianKernel"],
        help="kernel class to use for the estimator",
    )
    parser.add_argument(
        "--feature_map",
        action=FeatureMapAction,
        default=config.feature_map,
        choices=[
            "LinearTruncatedFourierFeatureMap",
            "ConstantTruncatedFourierFeatureMap",
            "LogTruncatedFourierFeatureMap",
        ],
        help="feature map class to use for transformation or a callable that returns a feature map",
    )
    parser.add_argument(
        "--optimiser",
        action=OptimiserAction,
        default=config.optimiser,
        choices=[
            "GurobiOptimiser",
            "AlglibOptimiser",
            "HighsOptimiser",
            "SoplexOptimiser",
        ],
        help="optimiser class to use for the optimization",
    )

    # Deprecated arguments
    # TODO: remove
    parser.add_argument(
        "--constant-lattice-points",
        action="store_true",
        help="flag to indicate whether to use a constant number of lattice points. Deprecated",
    )

    return parser
